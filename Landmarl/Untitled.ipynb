{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1b36f2-2900-418f-93ae-d6bb3a871b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting numpy>=1.21.2 (from opencv-python)\n",
      "  Using cached numpy-2.2.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached numpy-2.2.1-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-2.2.1 opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cb3b55-fb71-40fc-bdff-bb17f8404ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.20-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.4.38-cp310-cp310-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting numpy<2 (from mediapipe)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting sentencepiece (from mediapipe)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting ml_dtypes>=0.4.0 (from jax->mediapipe)\n",
      "  Using cached ml_dtypes-0.5.0-cp310-cp310-win_amd64.whl.metadata (22 kB)\n",
      "Collecting opt_einsum (from jax->mediapipe)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting scipy>=1.10 (from jax->mediapipe)\n",
      "  Using cached scipy-1.15.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe)\n",
      "  Using cached fonttools-4.55.3-cp310-cp310-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sophi\\onedrive\\dokumente\\htl\\5ahitm\\diplomarbeit\\gestures\\landmarl\\landmark\\lib\\site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib->mediapipe)\n",
      "  Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sophi\\onedrive\\dokumente\\htl\\5ahitm\\diplomarbeit\\gestures\\landmarl\\landmark\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sophi\\onedrive\\dokumente\\htl\\5ahitm\\diplomarbeit\\gestures\\landmarl\\landmark\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Downloading mediapipe-0.10.20-cp310-cp310-win_amd64.whl (51.0 MB)\n",
      "   ---------------------------------------- 0.0/51.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/51.0 MB 13.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 3.1/51.0 MB 10.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 4.7/51.0 MB 8.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 8.1/51.0 MB 10.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 11.5/51.0 MB 12.0 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 15.2/51.0 MB 12.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 19.4/51.0 MB 13.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 22.8/51.0 MB 14.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 27.5/51.0 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 32.0/51.0 MB 15.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 35.9/51.0 MB 15.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 39.6/51.0 MB 16.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 41.7/51.0 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 43.8/51.0 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 46.4/51.0 MB 14.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 47.7/51.0 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  50.3/51.0 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  50.9/51.0 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.0/51.0 MB 13.4 MB/s eta 0:00:00\n",
      "Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading jax-0.4.38-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 21.0 MB/s eta 0:00:00\n",
      "Downloading jaxlib-0.4.38-cp310-cp310-win_amd64.whl (64.2 MB)\n",
      "   ---------------------------------------- 0.0/64.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.1/64.2 MB 15.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 6.3/64.2 MB 14.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 9.2/64.2 MB 14.6 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 13.4/64.2 MB 16.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 16.8/64.2 MB 16.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 20.4/64.2 MB 16.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 22.5/64.2 MB 15.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 25.7/64.2 MB 15.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 29.1/64.2 MB 15.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 33.6/64.2 MB 16.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 37.0/64.2 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 40.9/64.2 MB 16.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 45.1/64.2 MB 16.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 49.5/64.2 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 53.0/64.2 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 56.9/64.2 MB 16.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 61.3/64.2 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  64.0/64.2 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  64.0/64.2 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 64.2/64.2 MB 15.7 MB/s eta 0:00:00\n",
      "Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl (45.5 MB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.55.3-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading ml_dtypes-0.5.0-cp310-cp310-win_amd64.whl (211 kB)\n",
      "Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Downloading scipy-1.15.0-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "   ---------------------------------------- 0.0/43.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.7/43.9 MB 24.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.1/43.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.3/43.9 MB 19.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.4/43.9 MB 18.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 17.6/43.9 MB 17.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 21.2/43.9 MB 17.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 25.2/43.9 MB 17.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 29.1/43.9 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.5/43.9 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.6/43.9 MB 16.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.5/43.9 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/43.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.3/43.9 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.9/43.9 MB 15.3 MB/s eta 0:00:00\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sentencepiece, flatbuffers, pyparsing, pycparser, protobuf, pillow, opt_einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "Successfully installed CFFI-1.17.1 absl-py-2.1.0 attrs-24.3.0 contourpy-1.3.1 cycler-0.12.1 flatbuffers-24.12.23 fonttools-4.55.3 jax-0.4.38 jaxlib-0.4.38 kiwisolver-1.4.8 matplotlib-3.10.0 mediapipe-0.10.20 ml_dtypes-0.5.0 numpy-1.26.4 opencv-contrib-python-4.10.0.84 opt_einsum-3.4.0 pillow-11.1.0 protobuf-4.25.5 pycparser-2.22 pyparsing-3.2.1 scipy-1.15.0 sentencepiece-0.2.0 sounddevice-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26da266d-827e-43e4-9516-e3e486e62f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.0-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sophi\\onedrive\\dokumente\\htl\\5ahitm\\diplomarbeit\\gestures\\landmarl\\landmark\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sophi\\onedrive\\dokumente\\htl\\5ahitm\\diplomarbeit\\gestures\\landmarl\\landmark\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85ea237-39c2-4ca0-9925-ff57fadf478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a9082b-746c-45f8-984c-36ed56b8b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class 0\n",
      "Collecting data for class 1\n",
      "Collecting data for class 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 6\n",
    "dataset_size = 100\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for j in range(number_of_classes):\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
    "        os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
    "\n",
    "    print('Collecting data for class {}'.format(j))\n",
    "\n",
    "    done = False\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(25)\n",
    "        cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06cb96cd-ae8a-4b07-b909-6a7b40dadb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class 3\n",
      "Collecting data for class 4\n",
      "Collecting data for class 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 6\n",
    "dataset_size = 100\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Start loop from class 3\n",
    "for j in range(3, number_of_classes):  # Start from class 3\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
    "        os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
    "\n",
    "    print('Collecting data for class {}'.format(j))\n",
    "\n",
    "    done = False\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(25)\n",
    "        cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342a7adc-4afc-49da-8978-38a8cc319194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118a8a1-c20f-45ef-a9b8-05f4b77912d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "f = open('data1.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d754476b-f4f7-4119-950f-273cab4018fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}\n",
    "\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(labels_dict[int(dir_)])\n",
    "\n",
    "f = open('dataLab.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf2f0f5-a119-441f-aed7-7995e32af966",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f67ca7d-329a-40c1-bedc-2b30c964ece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abf150e-94e2-466d-9551-a123db583127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c31cf1-c274-4fbd-a495-b1b621568e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of samples were classified correctly !\n"
     ]
    }
   ],
   "source": [
    "data_dict = pickle.load(open('./dataLab.pickle', 'rb'))\n",
    "\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print('{}% of samples were classified correctly !'.format(score * 100))\n",
    "\n",
    "f = open('modelLab.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0acf8b-e50b-452b-897d-c6addb9d23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = pickle.load(open('./modelLab.p', 'rb'))\n",
    "model = model_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336eb652-ae5e-4fad-8002-8f712158bec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, predicted_character, (x1, y1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1.3\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     58\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m     60\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     65\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}\n",
    "while True:\n",
    "\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90199060-3195-4393-97a0-2e86ff5f33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Mediapipe Hands and other variables\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}\n",
    "\n",
    "# Replace this with your trained model\n",
    "# Example: model = joblib.load('path_to_your_model.pkl')\n",
    "# Make sure the model is trained with 42 features\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_data = []\n",
    "\n",
    "        # Collect data for each detected hand\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "            bbox_area = (max(x_coords) - min(x_coords)) * (max(y_coords) - min(y_coords))\n",
    "            hand_data.append((bbox_area, hand_landmarks))\n",
    "\n",
    "        # Select the largest hand based on bounding box area\n",
    "        hand_data.sort(key=lambda x: x[0], reverse=True)\n",
    "        largest_hand = hand_data[0][1]\n",
    "\n",
    "        # Draw landmarks for the largest hand\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            largest_hand,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "        # Extract features from the largest hand\n",
    "        for lm in largest_hand.landmark:\n",
    "            x_.append(lm.x)\n",
    "            y_.append(lm.y)\n",
    "\n",
    "        for lm in largest_hand.landmark:\n",
    "            data_aux.append(lm.x - min(x_))\n",
    "            data_aux.append(lm.y - min(y_))\n",
    "\n",
    "        # Calculate bounding box for the largest hand\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "        x2 = int(max(x_) * W) + 10\n",
    "        y2 = int(max(y_) * H) + 10\n",
    "\n",
    "        # Make a prediction if the model is loaded\n",
    "        if model:\n",
    "            prediction = model.predict([np.asarray(data_aux)])\n",
    "            predicted_character = prediction[0]\n",
    "\n",
    "            # Display the prediction\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Sign Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "708209dc-3066-4be3-b7dc-d280ab6986e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landmark",
   "language": "python",
   "name": "landmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
